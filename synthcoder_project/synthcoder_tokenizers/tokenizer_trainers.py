import logging
from tokenizers import (#decoders,
                        # models,
                        normalizers,
                        pre_tokenizers,
                        processors,
                        # processors,
                        # trainers,
                        Tokenizer,
                        Regex)
from typing import Union, NoReturn, Literal

from  synthcoder_project import synthcoder_config 
from synthcoder_project.setup_logger import logged, create_logger

logger = create_logger(module_name=__name__)

class TokenizerTrainer():
    """
    A class to train fully functional tokenizers with build-in normalization and pretokenization.
    """

    @logged()
    def __init__(self, 
                 model_type: str,
                 tokenizer_train_file: str,
                 tokenizer_file_path_to_save: str,
                 model_tokenizer: type,
                 tokenizer_trainer_class_name: str,
                 special_tokens: list[str],
                 unk_token: str,
                 vocab_size: int,
                 min_frequency: int,
                 strip_accents: bool,
                 do_lower_case: bool, 
                 unicode_normalizer: Union[str, None],
                 add_metaspace: bool,
                 whitespace_split: bool,
                 punctuation_split: bool,
                 pattern_split: Union[str, list[str], None],
                 split_behavior: Union[Literal["removed", "isolated", "merged_with_previous", "merged_with_next", "contiguous"], 
                                        list[Literal["removed", "isolated", "merged_with_previous", "merged_with_next", "contiguous"]]],
                 suffix_CharBPETokenizer: str,
                 **kwargs
                 ) -> None:
        
        """
        Initialises an object. Prepares a tokenizer from the Tokenizers library, configures normalisation, pretokenization and postprocessing.
        The tokenizer is trained based on the provided training file. 
        The created tokenizer object (Tokenizers library) is loaded into the tokenizer from the Transformers library, which is model/architecture specific (e.g. for BERT). 


        Parameters:
        ===========
        model_type: Str. The model type that is being using.
        tokenizer_train_file: Str. File to be used for the tokenizer training. 
        tokenizer_file_path_to_save: Str. Path under which Tokenizers tokenizer will save its settings.
        model_tokenizer: Class. Tokenizer class from the Transformers library (or based on the TRansformers library). This tokenizer needs to be model specific. 
        tokenizer_trainer_class_name: Str. Name of the class of the tokenizer from the Tokenizers library, to use as a tokenizer trainer. 
        special_tokens: List of strings. Special tokens to be used by the tokenizer.   
        unk_token: Str. Token to be used to replace the unknown to the trained tokenizer vocabulary. 
        vocab_size: Int. The desired maximum size of the tokenizer's vocabulary. 
        min_frequency: Int. The minimum frequency a pair of tokens should have in order to be merged nto a new token during the tokenizer training. 
        strip_accents: Bool. Whether to strip all accents from letters during normalisation.
        do_lower_case: Bool. Whether to lowercase the input when tokenizing.
        unicode_normalizer: Union[str, None]. Unicode normaliser. 
        add_metaspace: Bool. If True, the pre-tokenizer will replace any whitespace by the replacement character (currently using `_`). It then tries to split on these spaces.
        whitespace_split: Bool. If True, the pre-tokenizer will simply split on the whitespaces. Works like .split()
        punctuation_split: Bool. If True, the pre-tokenizer will split on punctuation as individual characters. The punctutation symbols will be then `isolated`.
        pattern_split: Union[str, list[str], None]. If True, the pre-tokenizer will split  using the provided pattern. The pattern will be then `removed`. 
        split_behavior: Union[str, list[str]]. The behaviour indicating what happens with the punctuation symbol or the pattern used for splitting during pre-tokenization.
        suffix_CharBPETokenizer: Str. Suffix to attach to the tokens generated by `CharBPETokenizer` indicating the end of the word. It is used only for `CharBPETokenizer`. 
        **kwargs
        
        Returns:
        ========
        None
        """
        self.model_type = model_type
        self.tokenizer_train_file = tokenizer_train_file
        self.tokenizer_file_path_to_save = tokenizer_file_path_to_save
        self.model_tokenizer = model_tokenizer
        self.tokenizer_trainer_class_name = tokenizer_trainer_class_name
        self.special_tokens = special_tokens
        self.unk_token = unk_token
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.strip_accents = strip_accents
        self.do_lower_case = do_lower_case
        self.unicode_normalizer = unicode_normalizer
        self.add_metaspace = add_metaspace
        self.whitespace_split = whitespace_split
        self.punctuation_split = punctuation_split
        self.pattern_split = pattern_split
        self.split_behavior = split_behavior
        self.suffix_CharBPETokenizer = suffix_CharBPETokenizer


        self.tokenizer_tr = self._initialise_tokenizer_trainer()
        self._add_normalizer()
        self._add_pre_tokenizer()
        self._train_tokenizer()
        self._add_post_processor()  # post processor has to be after training!

        self.tokenizer_tr.save(self.tokenizer_file_path_to_save)
        self.loaded_tokenizer = Tokenizer.from_file(self.tokenizer_file_path_to_save)
        self.tokenizer = self.model_tokenizer(tokenizer_object=self.loaded_tokenizer, **kwargs)  # Loading the trained "tokenizers" tokenizer into a "transformers" tokenizer. 

    @logged()
    def _initialise_tokenizer_trainer(self) -> Union[object, NoReturn]:
        """
        Initilialises an appropriate tokenizer from the Tokenizers library.
        SentencePieceUnigramTokenizer and ByteLevelBPETokenizer are initialised without the `unk_token`.

        Returns:
        ========
        Object. Initialised tokenizer from the Tokenizers library.
        """
        if self.tokenizer_trainer_class_name in ("SentencePieceUnigramTokenizer", "ByteLevelBPETokenizer") :
            tokenizer_tr = synthcoder_config.ALLOWED_TOKENIZER_TRAINERS[self.tokenizer_trainer_class_name]()             
        elif self.tokenizer_trainer_class_name in synthcoder_config.ALLOWED_TOKENIZER_TRAINERS:
            tokenizer_tr = synthcoder_config.ALLOWED_TOKENIZER_TRAINERS[self.tokenizer_trainer_class_name](unk_token="[UNK]")
        else:
            raise ValueError(f"The selected `tokenizer_trainer_class_name` is not allowed." 
                             f"The available tokenizer trainer are: {synthcoder_config.ALLOWED_TOKENIZER_TRAINERS}")
        return tokenizer_tr
    
    @logged()
    def _add_normalizer(self) -> Union[None, NoReturn]:
        """
        Adds a sequence of normaliser options to the tokenizer.

        Returns:
        ========
        None
        """
        sequence = []

        if self.unicode_normalizer:
            if self.unicode_normalizer not in synthcoder_config.NORMALIZERS:
                raise ValueError(f"{self.unicode_normalizer} is not a known unicode normalizer."
                                 f"Available are {synthcoder_config.NORMALIZERS.keys()}")
            sequence.append(synthcoder_config.NORMALIZERS[self.unicode_normalizer]())

        if self.strip_accents:
            sequence.append(normalizers.StripAccents())
        
        if self.do_lower_case:
            sequence.append(normalizers.Lowercase())

        self.tokenizer_tr.normalizer = normalizers.Sequence(sequence)

    @logged()
    def _add_pre_tokenizer(self) -> None:
        """
        Adds a sequence of pre-tokenizer options to the tokenizer.

        Returns:
        ========
        None
        """        
        sequence = []

        if self.whitespace_split:
            sequence.append(pre_tokenizers.WhitespaceSplit())
        if self.punctuation_split:
            sequence.append(pre_tokenizers.Punctuation(behavior=self.split_behavior))
        if self.pattern_split:
            if isinstance(self.pattern_split, list):
                assert isinstance(self.split_behavior, list), "`split_behavior` needs to be a list if `pattern_split` is provided as a list"
                assert len(self.pattern_split) == len(self.split_behavior), "len(pattern_split) != len(split_behavior)"
                
                for idx, pattern in enumerate(self.pattern_split):
                    sequence.append(pre_tokenizers.Split(pattern=Regex(pattern), behavior=self.split_behavior[idx]))

            else:
                sequence.append(pre_tokenizers.Split(pattern=Regex(self.pattern_split), behavior=self.split_behavior))       
        if self.add_metaspace:
            sequence.append(pre_tokenizers.Metaspace())


        self.tokenizer_tr.pre_tokenizer = pre_tokenizers.Sequence(sequence)

    @logged()
    def _add_post_processor(self) -> Union[None, NoReturn]:
        """
        Adds model specific postprocessing to the tokenization, e.g. specifies addition of [CLS] and [SEP] tokens.  
        
        Returns:
        ========
        None
        """
        if self.model_type in ("bert", "distilbert"):
            cls_token_id = self.tokenizer_tr.token_to_id("[CLS]")
            sep_token_id = self.tokenizer_tr.token_to_id("[SEP]")

            self.tokenizer_tr.post_processor = processors.TemplateProcessing(
            single=f"[CLS]:0 $A:0 [SEP]:0",
            pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
            special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
            )
        else:
            raise ValueError(f"The model type `{self.model_type}` is not supported by the current tokenization post processor")


    @logged()
    def _train_tokenizer(self) -> None:
        """
        Trains the tokenizer from the Tokenizers library.

        Returns:
        ========
        None
        """
        tokenizer_kwargs = {
            "files": self.tokenizer_train_file,
            "vocab_size": self.vocab_size,
            "show_progress": True,
            "special_tokens": self.special_tokens,
        }

        # Not all arguments are accepted by all tokenizers. 
        if self.tokenizer_trainer_class_name == "SentencePieceUnigramTokenizer":
            tokenizer_kwargs["unk_token"] = self.unk_token
            print(f"\n{self.tokenizer_trainer_class_name} does not take into account the provided minimum frequency.\n")
        elif self.tokenizer_trainer_class_name == "CharBPETokenizer":
            tokenizer_kwargs["suffix"] = self.suffix_CharBPETokenizer
            tokenizer_kwargs["min_frequency"] = self.min_frequency
        else:
            tokenizer_kwargs["min_frequency"] = self.min_frequency

        self.tokenizer_tr.train(**tokenizer_kwargs)

    @logged()
    def return_trained_tokenizer(self) -> object:
        """
        Returns trained and model specific tokenizer from the Transformers library.

        Returns:
        ========
        Object.
        """
        return self.tokenizer


